{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DDAD - Dense Depth for Autonomous Driving\n",
    "\n",
    "DDAD is a new autonomous driving benchmark from TRI (Toyota Research Institute) for long range (up to 250m) and dense depth estimation in challenging and diverse urban conditions. It contains monocular videos and accurate ground-truth depth (across a full 360 degree field of view) generated from high-density LiDARs mounted on a fleet of self-driving cars operating in a cross-continental setting. DDAD contains scenes from urban settings in the United States (San Francisco, Bay Area, Cambridge, Detroit, Ann Arbor) and Japan (Tokyo, Odaiba). This notebook will demonstrate a number of simple steps that will allow you to load and visualize the DDAD dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import PIL\n",
    "from IPython import display\n",
    "from matplotlib.cm import get_cmap\n",
    "\n",
    "from dgp.datasets.synchronized_dataset import SynchronizedSceneDataset\n",
    "from dgp.proto.ontology_pb2 import Ontology\n",
    "from dgp.utils.protobuf import open_pbobject\n",
    "from dgp.utils.visualization import visualize_semantic_segmentation_2d\n",
    "\n",
    "plasma_color_map = get_cmap('plasma')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define high level variables\n",
    "DDAD_TRAIN_VAL_JSON_PATH = '/data/datasets/ddad_train_val/ddad.json'\n",
    "DDAD_TEST_JSON_PATH = '/data/datasets/ddad_test/ddad_test.json'\n",
    "DATUMS = ['lidar'] + ['CAMERA_%02d' % idx for idx in [1, 5, 6, 7, 8, 9]] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DDAD Train split\n",
    "\n",
    "The training set contains 150 scenes with a total of 12650 individual samples (75900 RGB images)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddad_train = SynchronizedSceneDataset(\n",
    "    DDAD_TRAIN_VAL_JSON_PATH,\n",
    "    split='train',\n",
    "    datum_names=DATUMS,\n",
    "    generate_depth_from_datum='lidar'\n",
    ")\n",
    "print('Loaded DDAD train split containing {} samples'.format(len(ddad_train)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load a random sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_sample_idx = np.random.randint(len(ddad_train))\n",
    "sample = ddad_train[random_sample_idx] # scene[0] - lidar, scene[1:] - camera datums\n",
    "sample_datum_names = [datum['datum_name'] for datum in sample]\n",
    "print('Loaded sample {} with datums {}'.format(random_sample_idx, sample_datum_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize camera images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concat images and visualize\n",
    "images = [cam['rgb'].resize((192,120), PIL.Image.BILINEAR) for cam in sample[1:]]\n",
    "images = np.concatenate(images, axis=1)\n",
    "display.display(PIL.Image.fromarray(images))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize corresponding depths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize corresponding depths, if the depth has been projected into the camera images\n",
    "if 'depth' in sample[1].keys():\n",
    "    # Load and resize depth images\n",
    "    depths = [cv2.resize(cam['depth'], dsize=(192,120), interpolation=cv2.INTER_NEAREST) \\\n",
    "              for cam in sample[1:]]\n",
    "    # Convert to RGB for visualization\n",
    "    depths = [plasma_color_map(d)[:,:,:3] for d in depths]\n",
    "    depths = np.concatenate(depths, axis=1)\n",
    "    display.display(PIL.Image.fromarray((depths*255).astype(np.uint8)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Lidar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: this requires open3d\n",
    "import open3d as o3d\n",
    "\n",
    "# Get lidar point cloud from sample\n",
    "lidar_cloud = sample[0]['point_cloud']\n",
    "# Create open3d visualization objects\n",
    "o3d_colors = np.tile(np.array([0., 0., 0.]), (len(lidar_cloud), 1))\n",
    "o3d_cloud = o3d.geometry.PointCloud()\n",
    "o3d_cloud.points = o3d.utility.Vector3dVector(lidar_cloud)\n",
    "o3d_cloud.colors = o3d.utility.Vector3dVector(o3d_colors)\n",
    "# Visualize (Note: needs open3d, openGL and X server support)\n",
    "o3d.visualization.draw_geometries([o3d_cloud])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DDAD train with temporal context\n",
    "\n",
    "To also return temporally adjacent scenes, use `forward_context` and `backward_context`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Intantiate dataset with forward and backward context\n",
    "\n",
    "ddad_train_with_context = SynchronizedSceneDataset(\n",
    "    DDAD_TRAIN_VAL_JSON_PATH,\n",
    "    split='train',\n",
    "    datum_names=('CAMERA_01',),\n",
    "    generate_depth_from_datum='lidar',\n",
    "    forward_context=1, \n",
    "    backward_context=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize front camera images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load random sample\n",
    "# Note that when forward_context or backward_context is used, the loader returns a list of samples\n",
    "samples = ddad_train_with_context[np.random.randint(len(ddad_train))] \n",
    "front_cam_images = []\n",
    "for sample in samples:\n",
    "    front_cam_images.append(sample[0]['rgb'])\n",
    "# Resize images and visualize\n",
    "front_cam_images = [img.resize((192,120), PIL.Image.BILINEAR) for img in front_cam_images]\n",
    "front_cam_images = np.concatenate(front_cam_images, axis=1)\n",
    "display.display(PIL.Image.fromarray(front_cam_images))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DDAD Val split\n",
    "\n",
    "The validation set contains 50 scenes with a total of 3950 individual samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the val set\n",
    "ddad_val = SynchronizedSceneDataset(\n",
    "    DDAD_TRAIN_VAL_JSON_PATH,\n",
    "    split='val',\n",
    "    datum_names=DATUMS,\n",
    "    generate_depth_from_datum='lidar'\n",
    ")\n",
    "print('Loaded DDAD val split containing {} samples'.format(len(ddad_val)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the panoptic segmentation labels from the val set\n",
    "\n",
    "50 of the DDAD validation samples have panoptic segmentation annotations for the front camera images. These annotations can be used for detailed, per-class evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "ddad_val = SynchronizedSceneDataset(\n",
    "    DDAD_TRAIN_VAL_JSON_PATH,\n",
    "    split='val',\n",
    "    datum_names=('CAMERA_01',),\n",
    "    requested_annotations=('semantic_segmentation_2d', 'instance_segmentation_2d'),\n",
    "    only_annotated_datums=True\n",
    ")\n",
    "print('Loaded annotated samples from DDAD val split. Total samples: {}.'.format(len(ddad_val)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize the semantic segmentation labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load instance and semantic segmentation ontologies\n",
    "semseg_ontology = open_pbobject(ddad_val.scenes[0].ontology_files['semantic_segmentation_2d'], Ontology)\n",
    "instance_ontology = open_pbobject(ddad_val.scenes[0].ontology_files['instance_segmentation_2d'], Ontology)\n",
    "\n",
    "# Load random sample \n",
    "random_sample_idx = np.random.randint(len(ddad_val))\n",
    "sample = ddad_val[random_sample_idx]\n",
    "\n",
    "# Get image and sample  segmentation annotation from sample \n",
    "image = np.array(sample[0]['rgb'])\n",
    "semantic_segmentation_2d_annotation = sample [0]['semantic_segmentation_2d']\n",
    "sem_seg_image = visualize_semantic_segmentation_2d(\n",
    "        semantic_segmentation_2d_annotation, semseg_ontology, image=image, debug=False\n",
    ")\n",
    "\n",
    "# Visualize\n",
    "image = cv2.resize(image, dsize=(320,240), interpolation=cv2.INTER_NEAREST)\n",
    "sem_seg_image = cv2.resize(sem_seg_image, dsize=(320,240), interpolation=cv2.INTER_NEAREST)\n",
    "display.display(PIL.Image.fromarray(np.concatenate([image, sem_seg_image], axis=1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DDAD Test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddad_test = SynchronizedSceneDataset(\n",
    "    DDAD_TEST_JSON_PATH,\n",
    "    split='test',\n",
    "    datum_names=DATUMS,\n",
    "    generate_depth_from_datum='lidar'\n",
    ")\n",
    "print('Loaded DDAD test split containing {} samples'.format(len(ddad_test)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}